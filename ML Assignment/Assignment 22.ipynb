{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e0667e",
   "metadata": {},
   "source": [
    "1. Is there any way to combine five different models that have all been trained on the same training\n",
    "data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is\n",
    "the reason?\n",
    "\n",
    "The simplest way of combining classifier output is to allow each classifier to make its own prediction and then choose the plurality prediction as the “final” output. This simple voting scheme is easy to implement and easy to understand, but it does not always produce the best possible results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b7071",
   "metadata": {},
   "source": [
    "2. What&#39;s the difference between hard voting classifiers and soft voting classifiers?\n",
    "\n",
    "Hard-voting ensembles output the mode of the base classifiers' predictions, whereas soft-voting ensembles average predicted probabilities (or scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a7d28",
   "metadata": {},
   "source": [
    "3. Is it possible to distribute a bagging ensemble&#39;s training through several servers to speed up the\n",
    "process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all\n",
    "options.\n",
    "\n",
    "Is it possible to speed up training of a bagging ensemble by distributing it accross multiple servers? It is quite possible to speed up training of a bagging ensemble by distributing it across multiples servers, since each predictor in the ensemble is independant of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb251e",
   "metadata": {},
   "source": [
    "4. What is the advantage of evaluating out of the bag?\n",
    "\n",
    "No leakage of data: Since the model is validated on the OOB Sample, which means data hasn't been used while training the model in any way, so there isn't any leakage of data and henceforth ensures a better predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f1fdf",
   "metadata": {},
   "source": [
    "5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra\n",
    "randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random\n",
    "Forests?\n",
    "\n",
    "Extra Trees is like a Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0e648",
   "metadata": {},
   "source": [
    "6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training\n",
    "data?\n",
    "\n",
    "If your adaboost ensemble underfits the training data, which hyperparameters should you tweak and how? You can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c52e1c",
   "metadata": {},
   "source": [
    "7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the\n",
    "training set?\n",
    "\n",
    "If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
