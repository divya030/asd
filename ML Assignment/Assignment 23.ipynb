{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5faad0d",
   "metadata": {},
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
    "disadvantages?\n",
    "\n",
    "To speed up a subsequent training algorithm (in some cases it may even remove noise and redundant features, making the training algorithm perform better). To visualize the data and gain insights on the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96141a43",
   "metadata": {},
   "source": [
    "2. What is the dimensionality curse?\n",
    "\n",
    "the curse of dimensionality references increasing data dimensions and its explosive tendencies. This phenomenon typically results in an increase in computational efforts required for its processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40becc0",
   "metadata": {},
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
    "can you go about doing it? If not, what is the reason?\n",
    "\n",
    "No, dimensionality reduction is not reversible in general. It loses information. Dimensionality reduction (compression of information) is reversible in auto-encoders. Auto-encoder is regular neural network with bottleneck layer in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43edf8",
   "metadata": {},
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "\n",
    "Yes, PCA can be used to significantly reduce dimensionality of highly non-linear dataset because it can at least get rid of useless dimensions. If there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa95e8d",
   "metadata": {},
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "\n",
    "Vanilla PCA: the dataset fit in memory\n",
    "    \n",
    "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "    \n",
    "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "    \n",
    "kenrl PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9ef2f",
   "metadata": {},
   "source": [
    "7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?\n",
    "\n",
    "The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c8072",
   "metadata": {},
   "source": [
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "\n",
    "It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
